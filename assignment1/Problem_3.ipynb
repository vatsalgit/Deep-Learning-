{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 3: Training and Fine-tuning on Fashion MNIST and MNIST\n",
    "Training neural networks with a huge number of parameters on a small dataset greatly affects the networks' generalization ability, often resulting in overfitting. Therefore, more often in practice, one would fine-tune existing networks that are trained on a larger dataset by continuing training on a smaller dataset. To get familiar with the fine-tuning procedure, in this problem you need to train a model from scratch on Fashion MNIST dataset and then fine-tune it on MNIST dataset. Note that we are training models on these two toy datasets because of limited computational resources. In most cases, we train models on ImageNet and fine-tune them on smaller datasets.\n",
    "\n",
    "* <b>Learning Objective:</b> In Problem 2, you implemented a covolutional neural network to perform classification task in TensorFlow. In this part of the assignment, we will show you how to use TensorFlow to fine-tune a trained network on a different task.\n",
    "* <b>Provided Codes:</b> We provide the the dataset downloading and preprocessing codes, conv2d(), and fc() functions to build the model performing the fine-tuning task.\n",
    "* <b>TODOs:</b> Train a model from scratch on Fashion MNIST dataset and then fine-tune it on MNIST dataset. Both the training loss and the training accuracy need to be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def download_data(download_root='data/', dataset='mnist'):\n",
    "    if dataset == 'mnist':\n",
    "        data_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    elif dataset == 'fashion_mnist':\n",
    "        data_url = 'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/'\n",
    "    else:\n",
    "        raise ValueError('Please specify mnist or fashion_mnist.')\n",
    "\n",
    "    data_dir = osp.join(download_root, dataset)\n",
    "    if osp.exists(data_dir):\n",
    "        print('The dataset was downloaded.')\n",
    "        return\n",
    "    else:\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    keys = ['train-images-idx3-ubyte.gz', 't10k-images-idx3-ubyte.gz',\n",
    "            'train-labels-idx1-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "\n",
    "    for k in keys:\n",
    "        url = (data_url+k).format(**locals())\n",
    "        target_path = osp.join(data_dir, k)\n",
    "        cmd = ['curl', url, '-o', target_path]\n",
    "        print('Downloading ', k)\n",
    "        subprocess.call(cmd)\n",
    "        cmd = ['gzip', '-d', target_path]\n",
    "        print('Unzip ', k)\n",
    "        subprocess.call(cmd)\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    num_train = 60000\n",
    "    num_test = 10000\n",
    "\n",
    "    def load_file(filename, num, shape):\n",
    "        fd = open(osp.join(data_dir, filename))\n",
    "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "        return loaded[num:].reshape(shape).astype(np.float)\n",
    "\n",
    "    train_image = load_file('train-images-idx3-ubyte', 16, (num_train, 28, 28, 1))\n",
    "    train_label = load_file('train-labels-idx1-ubyte', 8, num_train)\n",
    "    test_image = load_file('t10k-images-idx3-ubyte', 16, (num_test, 28, 28, 1))\n",
    "    test_label = load_file('t10k-labels-idx1-ubyte', 8, num_test)\n",
    "    return train_image, train_label, test_image, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download MNIST and Fashion MNIST\n",
    "download_data(dataset='mnist')\n",
    "download_data(dataset='fashion_mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def conv2d(input, output_shape, k=4, s=2, name='conv2d'):\n",
    "    with tf.variable_scope(name):\n",
    "        return slim.conv2d(input, output_shape, [k, k], stride=s)\n",
    "\n",
    "\n",
    "def fc(input, output_shape, act_fn=tf.nn.relu, name='fc'):\n",
    "    with tf.variable_scope(name):\n",
    "        return slim.fully_connected(input, output_shape, activation_fn=act_fn)\n",
    "\n",
    "\n",
    "def train(batch_size=100, num_epoch=5, learning_rate=1e-5,\n",
    "          num_train=60000, num_test=10000):\n",
    "    sess = tf.InteractiveSession()\n",
    "    \n",
    "    # Build the model\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    Y = tf.placeholder(tf.int64, [None])\n",
    "    labels = tf.one_hot(Y, 10)\n",
    "    _ = conv2d(X, 32, name='conv1')\n",
    "    _ = conv2d(_, 64, name='conv2')\n",
    "    _ = conv2d(_, 256, name='conv3')\n",
    "    _ = tf.reshape(_, [-1, np.prod(_.get_shape().as_list()[1:])])\n",
    "    _ = fc(_, 256, name='fc1')\n",
    "    logits = fc(_, 10, act_fn=None, name='fc2')\n",
    "\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "    loss_op = tf.reduce_mean(loss)\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    predict = tf.argmax(logits, 1)\n",
    "    correct = tf.equal(predict, Y)\n",
    "    accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    total_loss = []\n",
    "    total_accuracy = []\n",
    "\n",
    "    print('\\033[93mTrain Fashion MNIST\\033[0m')\n",
    "    X_train, Y_train, X_test, Y_test = load_data('data/fashion_mnist')\n",
    "    #############################################################################\n",
    "    # TODO: Train the model on Fashion MNIST from scratch                       #\n",
    "    # and then fine-tune it on MNIST                                            #\n",
    "    # Collect the training loss and the training accuracy                       #\n",
    "    # fetched from each iteration                                               #\n",
    "    # After the two stages of the training, the length of                       #\n",
    "    # total_loss and total_accuracy shuold be                                   #\n",
    "    # 2 *num_epoch * num_train / batch_size = 2 * 5 * 60000 / 100 = 6000        #\n",
    "    #############################################################################\n",
    "    # Train the model on Fashion MNIST\n",
    "    for epoch in range(num_epoch):\n",
    "        for i in range(num_train // batch_size):\n",
    "            pass\n",
    "        print('[Epoch {}] loss: {}, accuracy: {}'.format(epoch, loss, accuracy))\n",
    "\n",
    "    # Train the model on MNIST\n",
    "    print('\\033[93mTrain MNIST\\033[0m')\n",
    "    X_train, Y_train, X_test, Y_test = load_data('data/mnist')\n",
    "    for epoch in range(num_epoch):\n",
    "        for i in range(num_train // batch_size):\n",
    "            pass\n",
    "        print('[Epoch {}] loss: {}, accuracy: {}'.format(epoch, loss, accuracy))\n",
    "\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return total_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss, accuracy = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the training loss and the training accuracy\n",
    "plt.plot(loss)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.show()    \n",
    "\n",
    "plt.plot(accuracy)\n",
    "plt.title('training accuracy')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
