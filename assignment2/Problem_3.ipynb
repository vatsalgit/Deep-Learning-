{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [BONUS] Problem 3: Style Transfer\n",
    "NOTE: THIS PROBLEM IS A BONUS PROBLEM WHICH IS NOT REQUIRED.\n",
    "\n",
    "In this notebook we will implement the style transfer technique from \"[A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576)\".\n",
    "\n",
    "Read the paper first before starting doing the assginment!\n",
    "\n",
    "Also make sure you spare enough time for running the code. Sections after 'Finally! Run!' shouldn't take much time coding, but **does need ~3 hours to process** if you don't have tensorflow-gpu enabled. You can finish all sections once and leave it there for runing. Take your time.\n",
    "\n",
    "The general idea is to take two images, and produce a new image that reflects the content of one but the artistic \"style\" of the other. We will do this by first formulating a loss function that matches the content and style of each respective image in the feature space of a deep network, and then performing gradient descent on the pixels of the image itself.\n",
    "\n",
    "Please follow the assignment guide to setup python enviroments such as TensorFlow, Scipy, and Numpy.\n",
    "\n",
    "* <b>Learning Objective:</b> In this assignment, we will show you how to implement a basic style-transfer model in tensorflow.\n",
    "* <b>Provided Codes:</b> We provide the code framework for loading and processing a pre-trained CNN model.\n",
    "* <b>TODOs:</b> Design the loss functions and processes of a style tranfer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import what we need\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf  # Import TensorFlow after Scipy or Scipy will break\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We need to take the advatange of a CNN structure which (implicitly) understands image contents and styles. Rather than training a completely new model from scratch, we will use a pre-trained model to achieve our purpose - called \"transfer learning\".\n",
    "\n",
    "We will use the VGG19 model. Since the model itself is very large (>500Mb) you will need to download the [VGG-19 model](http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat) and put it under the `model/` folder. The comments below describes the dimensions of the VGG19 model. We will replace the max pooling layers with average pooling layers as the paper suggests, and discard all fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pick the VGG 19-layer model by from the paper \"Very Deep Convolutional \n",
    "# Networks for Large-Scale Image Recognition\".\n",
    "VGG_MODEL = 'model/imagenet-vgg-verydeep-19.mat'\n",
    "\n",
    "# The mean to subtract from the input to the VGG model. This is the mean that\n",
    "# when the VGG was used to train. Minor changes to this will make a lot of\n",
    "# difference to the performance of model.\n",
    "MEAN_VALUES = np.array([123.68, 116.779, 103.939]).reshape((1,1,3))\n",
    "\n",
    "def load_vgg_model(path):\n",
    "    \"\"\"\n",
    "    Returns a model for the purpose of 'painting' the picture.\n",
    "    Takes only the convolution layer weights and wrap using the TensorFlow\n",
    "    Conv2d, Relu and AveragePooling layer. VGG actually uses maxpool but\n",
    "    the paper indicates that using AveragePooling yields better results.\n",
    "    The last few fully connected layers are not used.\n",
    "    Here is the detailed configuration of the VGG model:\n",
    "        0 is conv1_1 (3, 3, 3, 64)\n",
    "        1 is relu\n",
    "        2 is conv1_2 (3, 3, 64, 64)\n",
    "        3 is relu    \n",
    "        4 is maxpool\n",
    "        5 is conv2_1 (3, 3, 64, 128)\n",
    "        6 is relu\n",
    "        7 is conv2_2 (3, 3, 128, 128)\n",
    "        8 is relu\n",
    "        9 is maxpool\n",
    "        10 is conv3_1 (3, 3, 128, 256)\n",
    "        11 is relu\n",
    "        12 is conv3_2 (3, 3, 256, 256)\n",
    "        13 is relu\n",
    "        14 is conv3_3 (3, 3, 256, 256)\n",
    "        15 is relu\n",
    "        16 is conv3_4 (3, 3, 256, 256)\n",
    "        17 is relu\n",
    "        18 is maxpool\n",
    "        19 is conv4_1 (3, 3, 256, 512)\n",
    "        20 is relu\n",
    "        21 is conv4_2 (3, 3, 512, 512)\n",
    "        22 is relu\n",
    "        23 is conv4_3 (3, 3, 512, 512)\n",
    "        24 is relu\n",
    "        25 is conv4_4 (3, 3, 512, 512)\n",
    "        26 is relu\n",
    "        27 is maxpool\n",
    "        28 is conv5_1 (3, 3, 512, 512)\n",
    "        29 is relu\n",
    "        30 is conv5_2 (3, 3, 512, 512)\n",
    "        31 is relu\n",
    "        32 is conv5_3 (3, 3, 512, 512)\n",
    "        33 is relu\n",
    "        34 is conv5_4 (3, 3, 512, 512)\n",
    "        35 is relu\n",
    "        36 is maxpool\n",
    "        37 is fullyconnected (7, 7, 512, 4096)\n",
    "        38 is relu\n",
    "        39 is fullyconnected (1, 1, 4096, 4096)\n",
    "        40 is relu\n",
    "        41 is fullyconnected (1, 1, 4096, 1000)\n",
    "        42 is softmax\n",
    "    \"\"\"\n",
    "    vgg = scipy.io.loadmat(path)\n",
    "\n",
    "    vgg_layers = vgg['layers']\n",
    "    def _weights(layer, expected_layer_name):\n",
    "        \"\"\"\n",
    "        Return the weights and bias from the VGG model for a given layer.\n",
    "        \"\"\"\n",
    "        W = vgg_layers[0][layer][0][0][2][0][0]\n",
    "        b = vgg_layers[0][layer][0][0][2][0][1]\n",
    "        layer_name = vgg_layers[0][layer][0][0][0]\n",
    "        assert layer_name == expected_layer_name\n",
    "        return W, b\n",
    "\n",
    "    def _relu(conv2d_layer):\n",
    "        \"\"\"\n",
    "        Return the RELU function wrapped over a TensorFlow layer. Expects a\n",
    "        Conv2d layer input.\n",
    "        \"\"\"\n",
    "        return tf.nn.relu(conv2d_layer)\n",
    "\n",
    "    def _conv2d(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D layer using the weights, biases from the VGG\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        W, b = _weights(layer, layer_name)\n",
    "        W = tf.constant(W)\n",
    "        b = tf.constant(np.reshape(b, (b.size)))\n",
    "        return tf.nn.conv2d(\n",
    "            prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME') + b\n",
    "\n",
    "    def _conv2d_relu(prev_layer, layer, layer_name):\n",
    "        \"\"\"\n",
    "        Return the Conv2D + RELU layer using the weights, biases from the VGG\n",
    "        model at 'layer'.\n",
    "        \"\"\"\n",
    "        return _relu(_conv2d(prev_layer, layer, layer_name))\n",
    "\n",
    "    def _avgpool(prev_layer):\n",
    "        \"\"\"\n",
    "        Return the AveragePooling layer.\n",
    "        \"\"\"\n",
    "        return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Constructs the graph model.\n",
    "    graph = {}\n",
    "    graph['input']   = tf.Variable(np.zeros((1, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS)), dtype = 'float32')\n",
    "    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')\n",
    "    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')\n",
    "    graph['avgpool1'] = _avgpool(graph['conv1_2'])\n",
    "    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')\n",
    "    graph['conv2_2']  = _conv2d_relu(graph['conv2_1'], 7, 'conv2_2')\n",
    "    graph['avgpool2'] = _avgpool(graph['conv2_2'])\n",
    "    graph['conv3_1']  = _conv2d_relu(graph['avgpool2'], 10, 'conv3_1')\n",
    "    graph['conv3_2']  = _conv2d_relu(graph['conv3_1'], 12, 'conv3_2')\n",
    "    graph['conv3_3']  = _conv2d_relu(graph['conv3_2'], 14, 'conv3_3')\n",
    "    graph['conv3_4']  = _conv2d_relu(graph['conv3_3'], 16, 'conv3_4')\n",
    "    graph['avgpool3'] = _avgpool(graph['conv3_4'])\n",
    "    graph['conv4_1']  = _conv2d_relu(graph['avgpool3'], 19, 'conv4_1')\n",
    "    graph['conv4_2']  = _conv2d_relu(graph['conv4_1'], 21, 'conv4_2')\n",
    "    graph['conv4_3']  = _conv2d_relu(graph['conv4_2'], 23, 'conv4_3')\n",
    "    graph['conv4_4']  = _conv2d_relu(graph['conv4_3'], 25, 'conv4_4')\n",
    "    graph['avgpool4'] = _avgpool(graph['conv4_4'])\n",
    "    graph['conv5_1']  = _conv2d_relu(graph['avgpool4'], 28, 'conv5_1')\n",
    "    graph['conv5_2']  = _conv2d_relu(graph['conv5_1'], 30, 'conv5_2')\n",
    "    graph['conv5_3']  = _conv2d_relu(graph['conv5_2'], 32, 'conv5_3')\n",
    "    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')\n",
    "    graph['avgpool5'] = _avgpool(graph['conv5_4'])\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Images\n",
    "\n",
    "Here we define some constants for the inputs. For this notebook, we will be uisng RGB images with 640 x 480 resolution, but you can easily modify the code to accommodate different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output folder for the images.\n",
    "OUTPUT_DIR = 'output/'\n",
    "# Style image to use.\n",
    "STYLE_IMAGE = 'images/muse.jpg'\n",
    "# Content image to use.\n",
    "CONTENT_IMAGE = 'images/trojan_shrine.jpg'\n",
    "# Image dimensions constants. \n",
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 480\n",
    "COLOR_CHANNELS = 3\n",
    "\n",
    "def load_image(path):\n",
    "    image_raw = scipy.misc.imread(path)\n",
    "    # Resize the image for convnet input and add an extra dimension\n",
    "    image_raw = scipy.misc.imresize(image_raw, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "    # Input to the VGG model expects the mean to be subtracted.\n",
    "    #############################################################################\n",
    "    # TODO: Substract the image with mean value                                 #\n",
    "    #############################################################################\n",
    "    image = None\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return [image_raw, image]\n",
    "\n",
    "def recover_image(image):\n",
    "    #############################################################################\n",
    "    # TODO: Recover the image with mean value                                   #\n",
    "    # HINT: Check value boundaries                                              #\n",
    "    #############################################################################\n",
    "    image_raw = None\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    return image_raw\n",
    "\n",
    "def save_image(path, image):\n",
    "    # Output should add back the mean.\n",
    "    image = recover_image(image)\n",
    "    scipy.misc.imsave(path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the input images. The vgg model expects image data with MEAN_VALUES subtracted to function correctly. \"load_image\" already handles this. The subtracted images will look funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[content_image_raw, content_image] = load_image(CONTENT_IMAGE)\n",
    "[style_image_raw, style_image] = load_image(STYLE_IMAGE)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))   \n",
    "ax1 = plt.subplot(221)\n",
    "ax2 = plt.subplot(222)\n",
    "ax3 = plt.subplot(223)\n",
    "ax4 = plt.subplot(224)\n",
    "\n",
    "ax1.imshow(content_image_raw)\n",
    "ax1.set_title('Content Image')\n",
    "ax2.imshow(content_image)\n",
    "ax2.set_title('Content Image Subtracted')\n",
    "ax3.imshow(style_image_raw)\n",
    "ax3.set_title('Style Image')\n",
    "ax4.imshow(style_image)\n",
    "ax4.set_title('Style Image Subtracted')\n",
    "\n",
    "# Show the resulting image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Image Generator\n",
    "\n",
    "The first step of style tranfer is to generate a starting image. The model will then gradually adjust this starting image towards target content/style. We will need a random image generator.\n",
    "\n",
    "The generated image can be arbitrary and doesn't necessarily have anything to do with the content image. But, generating something similar to the content image will reduce our computing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_noise_image(content_image, noise_ratio):\n",
    "    \"\"\"\n",
    "    Returns a noise image intermixed with the content image at a certain ratio.\n",
    "    \"\"\"\n",
    "    #############################################################################\n",
    "    # TODO: Create a noise image which will be mixed with the content image     #\n",
    "    #############################################################################\n",
    "    noise_image = None\n",
    "    #############################################################################\n",
    "    #                             END OF YOUR CODE                              #\n",
    "    #############################################################################\n",
    "    #Take a weighted average of the values\n",
    "    gen_image = noise_image * noise_ratio + content_image * (1.0 - noise_ratio)\n",
    "    return gen_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check by visualize images you generated. Keep in mind `noise_ratio = 0.0` produces the original subtracted image, while `noise_ratio = 1.0` produces a complete random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))   \n",
    "ax1 = plt.subplot(221)\n",
    "ax2 = plt.subplot(222)\n",
    "ax3 = plt.subplot(223)\n",
    "ax4 = plt.subplot(224)\n",
    "gen_image = generate_noise_image(content_image, 0.0)\n",
    "ax1.imshow(gen_image)\n",
    "ax1.set_title('Noise ratio: 0.0')\n",
    "gen_image = generate_noise_image(content_image, 0.25)\n",
    "ax2.imshow(gen_image)\n",
    "ax2.set_title('Noise ratio: 0.25')\n",
    "gen_image = generate_noise_image(content_image, 0.50)\n",
    "ax3.imshow(gen_image)\n",
    "ax3.set_title('Noise ratio: 0.50')\n",
    "gen_image = generate_noise_image(content_image, 0.75)\n",
    "ax4.imshow(gen_image)\n",
    "ax4.set_title('Noise ratio: 0.75')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the visulized images are not necessarily more clear with lower `noise_ratio`.\n",
    "### Inline Question(No points, just something interesting if you're curious): Why does the image sometimes look sharper when added some intermediate level of noise?\n",
    "#### Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "Once we generate a new image, we would like to evaluate it by how much it maintains contents while approaches the target style.\n",
    "This can be defined by a loss function. The loss function is a weighted sum of two terms: content loss + style loss. \n",
    "You'll fill in the functions that compute these weighted terms below.\n",
    "### Content Loss\n",
    "Let's first write the content loss function of equation (1) from the paper. Content loss measures how much the feature map of the generated image differs from the feature map of the source image. We only care about the content representation of one layer of the network (say, layer $\\ell$), that has feature maps $A^\\ell \\in \\mathbb{R}^{1 \\times H_\\ell \\times W_\\ell \\times N_\\ell}$. $N_\\ell$ is the number of filters/channels in layer $\\ell$, $H_\\ell$ and $W_\\ell$ are the height and width. We will work with reshaped versions of these feature maps that combine all spatial positions into one dimension. Let $F^\\ell \\in \\mathbb{R}^{M_\\ell \\times N_\\ell}$ be the feature map for the current image and $P^\\ell \\in \\mathbb{R}^{M_\\ell \\times N_\\ell}$ be the feature map for the content source image where $M_\\ell=H_\\ell\\times W_\\ell$ is the number of elements in each feature map. Each row of $F^\\ell$ or $P^\\ell$ represents the vectorized activations of a particular filter, convolved over all positions of the image.\n",
    "\n",
    "Then the content loss is given by:\n",
    "\n",
    "$L_c = \\frac{1}{2} \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2$\n",
    "\n",
    "We are only concerned with the \"conv4_2\" layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTENT_LAYER = 'conv4_2'\n",
    "\n",
    "def content_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Content loss function as defined in the paper.\n",
    "    \"\"\"\n",
    "    def _content_loss(current_feat, content_feat):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - current_feat: features of the current image, Tensor with shape [1, height, width, channels]\n",
    "        - content_feat: features of the content image, Tensor with shape [1, height, width, channels]\n",
    "\n",
    "        Returns:\n",
    "        - scalar content loss\n",
    "        \"\"\"\n",
    "        #############################################################################\n",
    "        # TODO: Compute content loss function                                       #\n",
    "        #############################################################################\n",
    "        loss = None\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return loss\n",
    "    return _content_loss(sess.run(model[CONTENT_LAYER]), model[CONTENT_LAYER])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Loss\n",
    "\n",
    "Now we can tackle the style loss of equation (5) from the paper. For a given layer $\\ell$, the style loss is defined as follows:\n",
    "\n",
    "First, compute the Gram matrix G which represents the correlations between the responses of each filter, where F is as above. The Gram matrix is an approximation to the covariance matrix -- we want the activation statistics of our generated image to match the activation statistics of our style image, and matching the (approximate) covariance is one way to do that. There are a variety of ways you could do this, but the Gram matrix is nice because it's easy to compute and in practice shows good results.\n",
    "\n",
    "Given a feature map $F^\\ell$ of shape $(1, M_\\ell,  N_\\ell)$, the Gram matrix has shape $(1, N_\\ell, N_\\ell)$ and its elements are given by:\n",
    "\n",
    "$$G_{ij}^\\ell  = \\sum_k F^{\\ell}_{ik} F^{\\ell}_{jk}$$\n",
    "\n",
    "Assuming $G^\\ell$ is the Gram matrix from the feature map of the current image, $A^\\ell$ is the Gram Matrix from the feature map of the source style image, then the style loss for the layer $\\ell$ is simply the Euclidean distance between the two Gram matrices:\n",
    "\n",
    "$$E_\\ell = \\frac{1}{4 N^2_\\ell M^2_\\ell} \\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n",
    "\n",
    "In practice we usually compute the style loss at a set of layers $\\mathcal{L}$ rather than just a single layer $\\ell$; then the total style loss is the weighted sum of style losses at each layer by $w_\\ell$:\n",
    "\n",
    "$$L_s = \\sum_{\\ell \\in \\mathcal{L}}  w_\\ell E_\\ell$$\n",
    "\n",
    "In our case it is a summation from conv1_1 (lower layer) to conv5_1 (higher layer). Intuitively, the style loss across multiple layers captures lower level features (hard strokes, points, etc) to higher level features (styles, patterns, even objects). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layers to use. We will use these layers as advised in the paper.\n",
    "# To have softer features, increase the weight of the higher layers\n",
    "# (conv5_1) and decrease the weight of the lower layers (conv1_1).\n",
    "# To have harder features, decrease the weight of the higher layers\n",
    "# (conv5_1) and increase the weight of the lower layers (conv1_1).\n",
    "STYLE_LAYERS = [\n",
    "    ('conv1_1', 0.5),\n",
    "    ('conv2_1', 0.5),\n",
    "    ('conv3_1', 0.5),\n",
    "    ('conv4_1', 0.5),\n",
    "    ('conv5_1', 0.5),\n",
    "]\n",
    "\n",
    "def style_loss_func(sess, model):\n",
    "    \"\"\"\n",
    "    Style loss function as defined in the paper.\n",
    "    \"\"\"\n",
    "    def _gram_matrix(feat):\n",
    "        \"\"\"\n",
    "        Compute the Gram matrix from features.\n",
    "\n",
    "        Inputs:\n",
    "        - feat: Tensor of shape (1, H, W, C) giving features for a single image.\n",
    "\n",
    "        Returns:\n",
    "        - gram: Tensor of shape (C, C) giving the (optionally normalized) Gram matrices for the input image.\n",
    "        \"\"\"\n",
    "        #############################################################################\n",
    "        # TODO: Compute gram matrix                                                 #\n",
    "        #############################################################################\n",
    "        gram = None\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return gram\n",
    "    \n",
    "    def _style_loss(current_feat, style_feat):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - current_feat: features of the current image, Tensor with shape [1, height, width, channels]\n",
    "        - style_feat: features of the style image, Tensor with shape [1, height, width, channels]\n",
    "\n",
    "        Returns:\n",
    "        - scalar style loss\n",
    "        \"\"\"\n",
    "        assert (current_feat.shape == style_feat.shape)\n",
    "        #############################################################################\n",
    "        # TODO: Compute style loss function                                         #\n",
    "        # HINT: Call the _gram_matrix function you just finished                    #\n",
    "        #############################################################################\n",
    "        loss = None\n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "        return loss\n",
    "\n",
    "    E = [_style_loss(sess.run(model[layer_name]), model[layer_name]) for layer_name, _ in STYLE_LAYERS]\n",
    "    W = [w for _, w in STYLE_LAYERS]\n",
    "    loss = sum([W[l] * E[l] for l in range(len(STYLE_LAYERS))])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an TensorFlow session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load VGG model\n",
    "model = load_vgg_model(VGG_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct content_loss using content_image.\n",
    "content_image_list = np.reshape(content_image, ((1,) + content_image.shape))\n",
    "sess.run(model['input'].assign(content_image_list))\n",
    "content_loss = content_loss_func(sess, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Construct style_loss using style_image.\n",
    "style_image_list = np.reshape(style_image, ((1,) + style_image.shape))\n",
    "sess.run(model['input'].assign(style_image_list))\n",
    "style_loss = style_loss_func(sess, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total loss\n",
    "$$L = \\alpha L_c + \\beta L_s$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constant to put more emphasis on content loss.\n",
    "ALPHA = 0.0025\n",
    "# Constant to put more emphasis on style loss.\n",
    "BETA = 1\n",
    "# Instantiate equation 7 of the paper.\n",
    "total_loss = ALPHA * content_loss + BETA * style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We minimize the total_loss, which is the equation 7.\n",
    "optimizer = tf.train.AdamOptimizer(2.0)\n",
    "train_step = optimizer.minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally! Run!\n",
    "Now we run the model which outputs the painted image every 100 iterations. You can find those intemediate results under `output/` folder. Notice on CPU it usually takes almost an hour to run 1000 iterations. Take your time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of iterations to run.\n",
    "ITERATIONS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "input_image = np.reshape(gen_image, ((1,) + gen_image.shape))\n",
    "sess.run(model['input'].assign(input_image))\n",
    "for it in range(ITERATIONS):\n",
    "    sess.run(train_step)\n",
    "    if it%50 == 0:\n",
    "        # Print every 100 iteration.\n",
    "        mixed_image = sess.run(model['input'])\n",
    "        print('Iteration %d' % (it))\n",
    "        print('cost: ', sess.run(total_loss))\n",
    "\n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "        filename = 'output/%d.png' % (it)\n",
    "        save_image(filename, mixed_image[0])\n",
    "        \n",
    "        final_image = recover_image(mixed_image[0]);\n",
    "        imshow(final_image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our final art for 500 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Assessments\n",
    "### Loss Function\n",
    "Now that we have some good results of our style transfer algorithm, we might want to take a deeper look at our settings.\n",
    "First of all, let's take a look at the total loss function: what does the ratio of content / style loss do with the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Change AlPHA to some value you desire                               #\n",
    "#############################################################################\n",
    "ALPHA1 = 0.025\n",
    "BETA1 = 1\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "# Instantiate equation 7 of the paper.\n",
    "total_loss_1 = ALPHA1 * content_loss + BETA1 * style_loss\n",
    "train_step_1 = optimizer.minimize(total_loss_1)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_image = np.reshape(gen_image, ((1,) + gen_image.shape))\n",
    "sess.run(model['input'].assign(input_image))\n",
    "for it in range(ITERATIONS):\n",
    "    sess.run(train_step_1)\n",
    "    if it%50 == 0:\n",
    "        # Print every 100 iteration.\n",
    "        mixed_image = sess.run(model['input'])\n",
    "        print('Iteration %d' % (it))\n",
    "        print('cost: ', sess.run(total_loss))\n",
    "\n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "        filename = 'output/%d.png' % (it)\n",
    "        save_image(filename, mixed_image[0])\n",
    "        \n",
    "        final_image = recover_image(mixed_image[0]);\n",
    "        imshow(final_image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question: Write down your insights on the roles of alpha and beta parameters.\n",
    "#### Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer's Hidden Information \n",
    "\n",
    "You might be wondering why we use 4th conv layer for contents and all 5 conv layers for style. Now change the parameters a little bit and run the code. See what's happening.\n",
    "\n",
    "Let's first see what does the content conv layer do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Change CONTENT_LAYER to one of the conv layers before conv4         #\n",
    "#############################################################################\n",
    "CONTENT_LAYER = 'conv2_2' # for example\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "content_loss_2 = content_loss_func(sess, model)\n",
    "total_loss_2 = ALPHA * content_loss_2 + BETA * style_loss\n",
    "train_step_2 = optimizer.minimize(total_loss_2)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_image = np.reshape(gen_image, ((1,) + gen_image.shape))\n",
    "sess.run(model['input'].assign(input_image))\n",
    "for it in range(ITERATIONS):\n",
    "    sess.run(train_step_2)\n",
    "    if it%50 == 0:\n",
    "        # Print every 100 iteration.\n",
    "        mixed_image = sess.run(model['input'])\n",
    "        print('Iteration %d' % (it))\n",
    "        print('cost: ', sess.run(total_loss))\n",
    "\n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "        filename = 'output/%d.png' % (it)\n",
    "        save_image(filename, mixed_image[0])\n",
    "        \n",
    "        final_image = recover_image(mixed_image[0]);\n",
    "        imshow(final_image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question: Write down your insights on the relation between depth of the layer and the content information of the image it represents. \n",
    "#### Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to change the style's representation. Reassgin the weights of each layer to values you desire.\n",
    "\n",
    "You can re run this single block multiple times to try out different values. Feel free to change ITERATIONS_3 of you find it's too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Change STYLE_LAYERS                                                 #\n",
    "#############################################################################\n",
    "ITERATIONS_3 = 500\n",
    "STYLE_LAYERS = [ # for example\n",
    "    ('conv1_1', 0.0),\n",
    "    ('conv2_1', 0.0),\n",
    "    ('conv3_1', 0.5),\n",
    "    ('conv4_1', 1.0),\n",
    "    ('conv5_1', 1.5),\n",
    "]\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "style_loss_3 = style_loss_func(sess, model)\n",
    "total_loss_3 = ALPHA * content_loss + BETA * style_loss_3\n",
    "train_step_3 = optimizer.minimize(total_loss_3)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "input_image = np.reshape(gen_image, ((1,) + gen_image.shape))\n",
    "sess.run(model['input'].assign(input_image))\n",
    "for it in range(ITERATIONS_3):\n",
    "    sess.run(train_step_3)\n",
    "    if it%(ITERATIONS / 10) == 0:\n",
    "        # Print every 100 iteration.\n",
    "        mixed_image = sess.run(model['input'])\n",
    "        print('Iteration %d' % (it))\n",
    "        print('cost: ', sess.run(total_loss))\n",
    "\n",
    "        if not os.path.exists(OUTPUT_DIR):\n",
    "            os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "        filename = 'output/%d.png' % (it)\n",
    "        save_image(filename, mixed_image[0])\n",
    "        \n",
    "        final_image = recover_image(mixed_image[0]);\n",
    "        imshow(final_image)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question: Write down your insights on the relation between weights of the layers and the style information they represent. \n",
    "#### Ans: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Question: Now, come to a conclusion on functions of each conv layer inside of a CNN network.\n",
    "#### Ans: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
